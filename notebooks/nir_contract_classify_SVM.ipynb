{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_files\n",
    "import re\n",
    "import glob\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import sys\n",
    "from functools import partial\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>12650</td>\n",
       "      <td>employment</td>\n",
       "      <td>##### \\n\\nExhibit 10.1 \\n\\n\\n\\nTHIS EMPLOYMENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>11980</td>\n",
       "      <td>employment</td>\n",
       "      <td>Exhibit 10.23\\n\\n\\n\\nThis Employment Agreement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>07020</td>\n",
       "      <td>employment</td>\n",
       "      <td>Exhibit 10.1 \\n\\nEXECUTION COPY \\n\\n\\n\\nThis E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>17250</td>\n",
       "      <td>employment</td>\n",
       "      <td>##### \\n\\n\\n\\nThis Agreement is made effective...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377</th>\n",
       "      <td>01340</td>\n",
       "      <td>SPA</td>\n",
       "      <td>\\n\\n\\n\\n\\n  \\n\\n\\nThis STOCK PURCHASE AGREEM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>12440</td>\n",
       "      <td>employment</td>\n",
       "      <td>\\n\\n\\nExhibit 10.1 Employment Agreement of M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>03590</td>\n",
       "      <td>employment</td>\n",
       "      <td>Exhibit 99.3\\n\\nConformed Executed Version\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>42420</td>\n",
       "      <td>bylaws</td>\n",
       "      <td>Exhibit 3.2  \\n  \\n\\n  \\nOF  \\n  \\nMERCARI COM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>02420</td>\n",
       "      <td>credit</td>\n",
       "      <td>Exhibit 4.12\\n\\n  \\n\\n\\n\\n\\nAND\\n\\nSTOCK PURCH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>44070</td>\n",
       "      <td>bylaws</td>\n",
       "      <td>EXHIBIT 3(i)\\n\\n\\n\\n  \\n\\n\\nOF\\n\\n  \\n\\n\\nHNI ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id       label                                            content\n",
       "1559  12650  employment  ##### \\n\\nExhibit 10.1 \\n\\n\\n\\nTHIS EMPLOYMENT...\n",
       "1492  11980  employment  Exhibit 10.23\\n\\n\\n\\nThis Employment Agreement...\n",
       "996   07020  employment  Exhibit 10.1 \\n\\nEXECUTION COPY \\n\\n\\n\\nThis E...\n",
       "2019  17250  employment  ##### \\n\\n\\n\\nThis Agreement is made effective...\n",
       "4377  01340         SPA    \\n\\n\\n\\n\\n  \\n\\n\\nThis STOCK PURCHASE AGREEM...\n",
       "1538  12440  employment    \\n\\n\\nExhibit 10.1 Employment Agreement of M...\n",
       "653   03590  employment  Exhibit 99.3\\n\\nConformed Executed Version\\n\\n...\n",
       "292   42420      bylaws  Exhibit 3.2  \\n  \\n\\n  \\nOF  \\n  \\nMERCARI COM...\n",
       "536   02420      credit  Exhibit 4.12\\n\\n  \\n\\n\\n\\n\\nAND\\n\\nSTOCK PURCH...\n",
       "457   44070      bylaws  EXHIBIT 3(i)\\n\\n\\n\\n  \\n\\n\\nOF\\n\\n  \\n\\n\\nHNI ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "with ZipFile(\"data.zip\") as z:\n",
    "    for fname in z.namelist():\n",
    "        if not fname.endswith('.txt') or not fname.startswith('data'):\n",
    "            continue\n",
    "        content = z.read(fname).decode('utf8')\n",
    "        label, idx = fname[5:-4].split('-', 1)\n",
    "        data.append((idx,label,content,))\n",
    "df_train = pd.DataFrame(data, columns=(\"id\", \"label\", \"content\"))\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train data\n",
    "#train_data = load_files(r'data', categories=['bylaws', 'credit', 'employment', 'rra', 'rsu', 'spa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = train_data.data, train_data.target_names\n",
    "#print(y[0])\n",
    "\n",
    "X = df_train['content'].to_list()\n",
    "\n",
    "y_data = df_train['label'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Txt preprocessing\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_data = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X_data = tfidfconverter.fit_transform(X_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "SVM_classifier = SVC(random_state=0)\n",
    "SVM_classifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = SVM_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.label.hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "\n",
    "data = []\n",
    "with ZipFile(\"test.zip\") as z:\n",
    "    for fname in z.namelist():\n",
    "        content = z.read(fname).decode('utf8')\n",
    "        if not fname.endswith('.txt') or not fname.startswith('test'):\n",
    "            continue\n",
    "        idx = fname[5:-4]\n",
    "        data.append((idx,content,))\n",
    "df_test = pd.DataFrame(data, columns=(\"id\", \"content\"))\n",
    "df_test.sample(10)\n",
    "\n",
    "X_test_data = df_test['content'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Txt preprocessing\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X_test_data)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X_test_data[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_test_data = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X_test_data = tfidfconverter.fit_transform(X_test_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = SVM_classifier.predict(X_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['label'] = y_pred\n",
    "print(df_test.label.hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from functools import partial\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "sys.path.append('../src')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "import json, random\n",
    "\n",
    "def submit(name,company,email,phone,user, test_data):\n",
    "    \"\"\"Submits a test data set to the pydata server, and returns accuracy\"\"\"\n",
    "    try:\n",
    "        test_data = {str(k).lower().replace(\".txt\", \"\"):str(v).lower().strip() for k,v in test_data.items()}\n",
    "        data = parse.urlencode({\"user\":user, \"submission\": json.dumps(test_data), \"name\":name,\"company\":company,\"phone\":phone,\"email\":email}).encode()\n",
    "        req =  request.Request(\"https://goren.ml/uattcontract/index.php?rand=\"+str(random.randint(167,1000000)), data=data)\n",
    "        resp = request.urlopen(req)\n",
    "        return float(resp.read().decode(\"utf8\"))\n",
    "    except Exception as e:\n",
    "        print (str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're having problem importing this file, copy and paste the contents of \"uatt.py\" in this cell instead\n",
    "# of the import statement\n",
    "#from uatt import submit\n",
    "\n",
    "my_submit = partial(submit, \"Nir Cohen\", \"IAI\", \"nir.cohen13@gmail.com\", \"0546840888\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = dict(df_test[[\"id\",\"label\"]].values)\n",
    "#submission name must be unique\n",
    "submission_name = \"Nir_Cohen_1\"\n",
    "my_submit(submission_name, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, string\n",
    "from IPython.display import HTML\n",
    "rand_str = lambda: \"\".join(random.sample(string.ascii_letters,7))\n",
    "HTML('<a href=\"https://goren.ml/uattcontract/?{k}={v}\" target=\"_blank\">Go to Leaderboard</a>'.format(k=rand_str(),v=rand_str()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "#with open('text_classifier', 'wb') as picklefile:\n",
    "#    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data\n",
    "\n",
    "#data = []\n",
    "#with ZipFile(\"test.zip\") as z:\n",
    "#    for fname in z.namelist():\n",
    "#        content = z.read(fname).decode('utf8')\n",
    "#        if not fname.endswith('.txt') or not fname.startswith('test'):\n",
    "#            continue\n",
    "#        idx = fname[5:-4]\n",
    "#        data.append((idx,content,))\n",
    "#df_test = pd.DataFrame(data, columns=(\"id\", \"content\"))\n",
    "#df_test.sample(10)\n",
    "\n",
    "#X_test = df_test['content'].to_list()\n",
    "\n",
    "\n",
    "#path = os.path.dirname('C:\\Users\\nirco\\contract_classification\\notebooks\\test')\n",
    "#csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#csv_files = glob.glob('test')\n",
    "\n",
    "#train_data = load_files(r'test')\n",
    "#X_test = train_data.data\n",
    "\n",
    "# List comprehension that loads of all the files\n",
    "# X_test = [pd.read_csv(x) for x in path]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
